{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6314b32b",
   "metadata": {},
   "source": [
    "# INDEX\n",
    "1. [Feature Engineering & Customer Insights](#feature-engineering--customer-insights)\n",
    "2. [Tools Used](#tools-used)\n",
    "4. [Architecture Overview](#architecture-overview-custom-modular-layer-based-architecture)\n",
    "5. [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)\n",
    "6. [Data Cleaning](#data-cleaning)\n",
    "    - Duplicate Checks\n",
    "    - üïì Timestamp Nulls and Fulfillment Gaps\n",
    "    - Data loss\n",
    "7. [Data Quality](#data-quality)\n",
    "    - Data Tests\n",
    "    - Source Freshness Checks\n",
    "8. [Sanity Checks](#sanity-checks)\n",
    "    - Sanity Check UnitPrice\n",
    "    - Sanity Check Quantity\n",
    "    - Sanity Check Product pricing consistency\n",
    "9. [Business Logic Exclusions](#business-logic-exclusions)\n",
    "10. [Data Transformation](#data-transformation)\n",
    "    - Macro Function\n",
    "    - How KPIS & models were built\n",
    "11. [Business Intelligence Dashboard](#business-intelligence-dashboard)\n",
    "12. [Data Modeling](#data-modeling)\n",
    "    - OBT (One big table)\n",
    "    - Star Schema\n",
    "    - Incremental models\n",
    "13. [Data Lineage](#data-lineage)\n",
    "14. [Data Orchestration](#data-orchestration)\n",
    "    - Cron jobs\n",
    "    - Partitioning\n",
    "15. [CI/CD](#ci-cd)\n",
    "    - Dockerfile\n",
    "    - Github Actions\n",
    "    - GCP Artifact Registery\n",
    "16. [Conclusion](#conclusion)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## üß© Problem Statement\n",
    "\n",
    "This project transforms raw Point-of-Sale (POS) transaction logs into actionable customer intelligence by engineering and segmenting key behavioral and financial metrics. The pipeline performs the following computations and classifications:\n",
    "\n",
    "### Feature Engineering & Customer Insights\n",
    "\n",
    "* **Average Order Value (AOV)**\n",
    "\n",
    "  * Per-customer AOV\n",
    "  * Overall AOV across all customers\n",
    "* **Customer Lifetime Value (CLTV)**\n",
    "\n",
    "  * Individual CLTV based on historical purchases and frequency\n",
    "  * Overall CLTV across the customer base\n",
    "  * CLTV-based segmentation (e.g. High, Medium, Low value)\n",
    "\n",
    "* **RFM Analysis**\n",
    "\n",
    "  * Computed Recency, Frequency, and Monetary metrics per customer\n",
    "  * RFM segmentation (e.g. champions, at risk, hibernating)\n",
    "* **Churn Prediction**\n",
    "\n",
    "  * Flag churned customers based on inactivity beyond a defined purchase window\n",
    "  * Churn Risk segmentation (e.g. Low, Medium, High Risk)\n",
    "\n",
    "* **Engagement Metrics**\n",
    "\n",
    "  * Daily Active Users (DAU)\n",
    "  * Monthly Active Users (MAU)\n",
    "  * Stickiness Ratio (DAU/MAU)\n",
    "\n",
    "### ‚úÖ Input Table (Truncated for Readability)\n",
    "| INVOICENO | STOCKCODE | DESCRIPTION                           | QUANTITY | INVOICEDATE           | UNITPRICE | CUSTOMERID | COUNTRY         |\n",
    "|-----------|-----------|---------------------------------------|----------|------------------------|-----------|------------|-----------------|\n",
    "| 536365    | 85123A    | WHITE HANGING HEART T-LIGHT HOLDER    | 6        | 2010-12-01 08:26:00   | 2.55      | 17850.0    | United Kingdom  |\n",
    "| 536365    | 71053     | WHITE METAL LANTERN                   | 6        | 2010-12-01 08:26:00   | 3.39      | 17850.0    | United Kingdom  |\n",
    "| 536365    | 84406B    | CREAM CUPID HEARTS COAT HANGER        | 8        | 2010-12-01 08:26:00   | 2.75      | 17850.0    | United Kingdom  |\n",
    "| ...       | ...       | ...                                   | ...      | ...                    | ...       | ...        | ...             |\n",
    "\n",
    "\n",
    "### ‚úÖ Output Table (Customer Summary ‚Äì Part 1)\n",
    "\n",
    "| CUSTOMERID | RECENCY | MONETARY | FREQUENCY | TENURE | R\\_SCORE | F\\_SCORE | M\\_SCORE |\n",
    "| ---------- | ------- | -------- | --------- | ------ | -------- | -------- | -------- |\n",
    "| 17850      | 303     | 5288.63  | 35        | 374    | 5        | 5        | 5        |\n",
    "\n",
    "‚û°Ô∏è *Continued below to fit on one page*\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Output Table (Customer Summary ‚Äì Part 2)\n",
    "\n",
    "| RFM\\_SEGMENTATION | CUSTOMER\\_CLTV\\_PROFIT | CLTV\\_PROFIT\\_SEGMENTATION | ISCHURNED | CHURN\\_RISK\\_GROUP |\n",
    "| ----------------- | ---------------------- | -------------------------- | --------- | ------------------ |\n",
    "| Champions / VIPs  | 133131.089811720       | High                       | true      | Churned and input  |\n",
    "\n",
    "\n",
    "\n",
    "### üéØ Why This Matters\n",
    "\n",
    "This solution supports data-driven decision-making in customer relationship management, by enabling:\n",
    "\n",
    "* **Customer Prioritization**: Identify and focus on high-value and at-risk segments\n",
    "* **Personalized Marketing**: Enable tailored campaigns based on behavior and value\n",
    "* **Retention Strategy**: Detect churn patterns early to re-engage slipping customers\n",
    "* **Revenue Optimization**: Adjust offers, pricing, and promotions based on customer metrics\n",
    "* **Executive Insights**: Provide KPIs for leadership to monitor customer health over time\n",
    "* **Product Feedback Loops**: Align marketing and product decisions with customer behavior trends\n",
    "\n",
    "---\n",
    "\n",
    "## Tools Used\n",
    "\n",
    "* **Python** ‚Äì Utilized for rapid prototyping, data wrangling, feature engineering, and initial transformation logic prior to dbt implementation\n",
    "* **SQL** ‚Äì Employed for exploratory analysis, ad hoc queries, and core transformations within dbt models\n",
    "* **dbt (Data Build Tool)** ‚Äì Used for scalable data transformations, implementing modular data models, enforcing data quality tests, CI/CD integration, and generating automated documentation\n",
    "* **Snowflake** ‚Äì Cloud data warehouse for centralized storage and compute, enabling scalable querying and analytics\n",
    "* **Plotly** ‚Äì Interactive visualization library used for generating rich, responsive charts and exploratory visual analysis\n",
    "* **Streamlit** ‚Äì Lightweight web framework for building the final interactive dashboard, enabling end-user engagement with KPIs and segmentation outputs\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview: Custom modular layer-based architecture \n",
    "\n",
    "## üèóÔ∏è Architecture Overview: Modular Layered Data Modeling with dbt\n",
    "\n",
    "To enable reliable, scalable, and transparent transformations, I designed and implemented a **custom modular layer-based architecture** using **dbt**, tailored specifically for eCommerce delivery and customer analytics.\n",
    "\n",
    "This architecture follows the **staging ‚Üí intermediate ‚Üí mart** pattern, ensuring clean separation of concerns, high reusability, and clear data lineage across the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Layer 1: **Staging (`stg_`)**\n",
    "\n",
    "This layer standardizes raw POS transaction data by cleaning, renaming, and typing columns.\n",
    "\n",
    "* **`stg_transaction_logs.sql`**:\n",
    "  Raw transaction data is ingested and standardized here before transformation logic is applied.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Layer 2: **Intermediate (`int_`)**\n",
    "\n",
    "This layer performs reusable business logic transformations, aggregations, and metric computations.\n",
    "\n",
    "**Key intermediate models:**\n",
    "\n",
    "* `int_customer_aov.sql` ‚Äì Computes per-customer Average Order Value\n",
    "* `int_customer_cltv.sql` ‚Äì Derives Customer Lifetime Value metrics\n",
    "* `int_customer_cltv_segmentation.sql` ‚Äì Segments customers by CLTV tiers\n",
    "* `int_customer_rfm_score.sql` ‚Äì Calculates Recency, Frequency, and Monetary scores\n",
    "* `int_customer_churn_flag.sql` ‚Äì Flags customers as churned based on purchasing windows\n",
    "* `int_customer_churn_segmentation.sql` ‚Äì Classifies churn risk segments\n",
    "* `int_customer_dau_mau_stickiness.sql` ‚Äì Calculates engagement metrics (DAU, MAU, Stickiness)\n",
    "\n",
    "These intermediate models are **modular** and **refactored**, allowing downstream mart models to pull exactly what they need with minimal coupling.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Layer 3: **Mart (`obt_`)**\n",
    "\n",
    "This layer creates analytics-ready tables for dashboards and reporting, often joined from multiple intermediate models.\n",
    "\n",
    "**Final data marts:**\n",
    "\n",
    "* `obt_customer_kpis_and_segmentation`\n",
    "\n",
    "  * Joins customer-level metrics from all intermediate models into one wide table\n",
    "  * Includes AOV, CLTV, RFM scores, churn flags, segmentation labels, and engagement metrics\n",
    "* `obt_overall_customer_kpis`\n",
    "\n",
    "  * Aggregates overall metrics across all customers (e.g., average CLTV, total DAUs)\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Joins & Relationships\n",
    "\n",
    "Each mart model is built using `ref()` to pull cleanly from the intermediate layer:\n",
    "\n",
    "* `obt_customer_kpis_and_segmentation` joins on `customer_id` across:\n",
    "\n",
    "  * `int_customer_aov`\n",
    "  * `int_customer_cltv`\n",
    "  * `int_customer_rfm_score`\n",
    "  * `int_customer_churn_flag`\n",
    "  * `int_customer_cltv_segmentation`\n",
    "  * `int_customer_churn_segmentation`\n",
    "  * `int_customer_dau_mau_stickiness`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why This Approach Works\n",
    "This structure supports **lineage tracking**, **test coverage**, and **CI/CD integration**, ensuring data trustworthiness from raw logs to dashboard-ready insights.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  Exploratory Data Analysis (EDA)\n",
    "\n",
    "* Inspected column names, data types, and overall schema for consistency\n",
    "* Verified completeness by checking for nulls, anomalies, and malformed records\n",
    "* Counted unique values per column to identify categorical features, keys, and potential grouping fields\n",
    "\n",
    "---\n",
    "\n",
    "##  Data Cleaning\n",
    "\n",
    "I performed systematic cleaning across all BRONZE tables to prepare the data for downstream merging and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÇ Source: Layer 1: **Staging (`stg_`)\n",
    "\n",
    "**Key Cleaning Actions:**\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Duplicate Checks\n",
    "\n",
    "* **No duplicates** found in stg_transaction_logs (raw transaction logs)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üïì Timestamp Nulls and Fulfillment Gaps\n",
    "\n",
    "**Columns affected by nulls:**\n",
    "\n",
    "| Column Name   | Null Count | Most Likely Reason (When Both Are Null)                                                                                                                                                                                                                                                                                                                                                          |\n",
    "| ------------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `Description` | 1,454      | When `Description` and `CustomerID` are both null, and `UnitPrice = 0.0`, these rows most likely represent **system-generated inventory adjustments** or **manual returns** where no product was properly recorded and no customer was involved. If `Quantity` is **negative**, it's likely a **write-off for lost/damaged stock**. If **positive**, it may reflect a **restock or test entry**. |\n",
    "| `CustomerID`  | 135,080    | For the subset where `CustomerID` and `Description` are both null, these are likely **non-customer-facing transactions** ‚Äî such as inventory corrections, returns without proper logging, or zero-value backend entries ‚Äî not intended to track customer behavior or product sales. If only CUstomerID null the purchase was made without a registered customer account ‚Äî likely a guest checkout. or if StockCode = DOT then non-product related item such as Postage costs.                                                                                                              |\n",
    "# Sanity Checks\n",
    "## 1. Sanity Check UnitPrice\n",
    "#### 1.1 Negative `UnitPrice` flagged (debt write-offs)\n",
    "EXAMPLE \n",
    "\tInvoiceNo\tStockCode\tDescription\tQuantity\tInvoiceDate\tUnitPrice\tCustomerID\tCountry\n",
    "299983\tA563186\tB\tAdjust bad debt\t1\t2011-08-12 14:51:00\t-11062.06\tNaN\tUnited Kingdom\n",
    "299984\tA563187\tB\tAdjust bad debt\t1\t2011-08-12 14:52:00\t-11062.06\tNaN\tUnited Kingdom\n",
    "\n",
    "#### 1.2 Unit Price != 0\n",
    "| Group Label                          | Keywords Trigger                                                               |\n",
    "| ------------------------------------ | ------------------------------------------------------------------------------ |\n",
    "| `Damaged or Written-Off`             | `damaged`, `damages`, `returned`, `given away`, `broken`                       |\n",
    "| `Inventory Adjustment`               | `adjustment`, `taig adjust`, `manual`, `website fixed`, `stock check`, `check` |\n",
    "| `Logistics (Dotcom / Amazon / eBay)` | `dotcom`, `amazon`, `ebay`, `mailout`                                          |\n",
    "| `Coding / Mapping Issues`            | `wrongly coded`, `incorrectly credited`, `Actual description`                  |\n",
    "| `Unknown / Incomplete`               | `??`, `missing`, `test`, `manual`, `?display?`                                 |\n",
    "| `Bundling / Set Handling`            | `sold as set`, `sold as sets`, `dotcom sold sets`, `amazon sold sets`          |\n",
    "| `Promotional / Internal Use`         | `showroom`, `display`, `samples`, `given away`                                 |\n",
    "\n",
    "\n",
    "## 2. Sanity Check Quantity\n",
    "\n",
    "üîÑ Why `Quantity` Is Negative: Business Logic Behind Returns & Adjustments\n",
    "\n",
    "In this e-commerce dataset, a **negative `Quantity`** typically indicates a **reverse transaction**, such as a **return, refund, discount, or stock correction**. These are not new sales ‚Äî they are credits or adjustments against previous sales.\n",
    "\n",
    "Below are the key scenarios that explain why quantity is `< 0`:\n",
    "\n",
    "‚úÖ  **I. Invoice Number Starts with 'C'**\n",
    "**Example**: `C2773`\n",
    "* This prefix conventionally stands for **Credit Note** ‚Äî a reversal of a previous invoice.\n",
    "* It indicates the customer was refunded or the sale was cancelled.\n",
    "* Negative quantity reflects the product being removed from revenue and potentially restocked.\n",
    "\n",
    "‚úÖ **II. StockCode Contains 'C'**\n",
    "**Example**: `C35004C`\n",
    "* These StockCodes often represent **returns**.\n",
    "* The \"C\" may be used internally to flag **credit items**.\n",
    "* Quantity is negative because it's offsetting a prior positive sale.\n",
    "\n",
    "‚úÖ **III. StockCode = 'D' and Description = 'Discount'**\n",
    "* These rows represent **line-item discounts** applied to orders.\n",
    "* The `Quantity` is set to `-1` (or another negative number) as a way of **applying a price reduction** in the sales log.\n",
    "\n",
    "‚úÖ **IV. StockCode = 'M' and Description = 'Manual'**\n",
    "* These are **manual price corrections** entered by staff.\n",
    "* Often used to fix pricing errors or override a transaction.\n",
    "* Negative quantity indicates a **reverse entry or credit** to offset the mistake.\n",
    "\n",
    "### üßæ Summary Table of reasons why quantity is `< 0`\n",
    "\n",
    "| Condition                                        | Interpretation                       |\n",
    "| ------------------------------------------------ | ------------------------------------ |\n",
    "| `InvoiceNo` starts with `\"C\"`                    | Credit note (full or partial refund) |\n",
    "| `StockCode` contains `\"C\"`                       | Return    |\n",
    "| `StockCode = \"D\"` and `Description = \"Discount\"` | Discount applied as a negative item  |\n",
    "| `StockCode = \"M\"` and `Description = \"Manual\"`   | Manual price correction by staff     |\n",
    "\n",
    "---\n",
    "## 3. Sanity Check Product pricing consistency\n",
    "\n",
    "To ensure data quality and reliable revenue analysis, I conducted a sanity check on product pricing consistency over time. Abrupt or erratic pricing behavior can signal either legitimate business logic (e.g. promotions) or data quality issues (e.g. manual errors, misentries). This check helps distinguish between the two.\n",
    "\n",
    "| **Symptom**              | **Date Behavior Pattern**                                                                                                                                                        |\n",
    "| ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Catalogue Revision**   | Price changes **once** and remains stable afterward. A clear ‚Äúbefore‚Äù and ‚Äúafter‚Äù period, indicating a formal catalogue update.                                                  |\n",
    "| **Promotion / Discount** | Temporary price drops that occur in **short bursts** (e.g., during Black Friday, seasonal sales). Prices return to normal afterward.                                             |\n",
    "| **Data Entry Error**     | A single, **isolated price anomaly** with no similar entries nearby. Usually occurs on a stray date and may reflect a manual input mistake.                                      |\n",
    "| **Zero Price / Manual**  | Scattered zero- or unusually low prices, often linked to **credit notes** (InvoiceNo starts with `\"C\"`) or **manual adjustments**. These often co-occur with returns or refunds. |\n",
    "\n",
    "To ensure pricing integrity across products, each SKU (Stock Keeping Unit) was assigned a flag based on how its `UnitPrice` behaved over time. This helped differentiate stable products from those affected by promotions, adjustments, or data issues.\n",
    "\n",
    "| **Flag Value**                   | **Count**    | **Definition / Pattern in Data**                                                                                     | **Typical Business Cause**                                                | **Recommended Treatment**                                                                                                       |\n",
    "| -------------------------------- | ------------ | -------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`normal`**                     | *All others* | SKU is consistently sold at a **single non-zero price** throughout the dataset.                                      | Standard catalogue pricing with no fluctuation.                           | Use as-is in all metrics and models (AOV, CLTV, pricing forecasts).                                                             |\n",
    "| **`oscillating_or_promotional`** | **1,046**    | SKU has **two or more recurring prices** ‚Äî usually a base price with intermittent discounts (e.g., seasonal sales).  | Time-bound promotions, temporary discounts, or customer-specific pricing. | Retain all prices. For demand modeling, tag discounted rows with a `\"promo\"` feature to isolate elastic demand behavior.        |\n",
    "| **`zero_or_manual`**             | **1,034**    | `UnitPrice = 0` or `StockCode = 'M'` / `Description = 'Manual'`. Often isolated and paired with `Quantity = 1`.      | Free samples, goodwill gestures, credit note balancing items.             | Exclude from price analyses; include in net revenue and refund logic (value = ¬£0).                                              |\n",
    "| **`one_off_price_change`**       | **425**      | Exactly **two distinct prices**, with a **clear, non-overlapping date split**. Example: ¬£6.35 until Jan, then ¬£1.95. | Permanent price revision due to catalogue update or product change.       | Split into pricing eras or retain the later price if forecasting forward-looking demand.                                        |\n",
    "| **`complex`**                    | **138**      | Irregular pricing: **more than two prices**, overlapping timeframes, and combinations of zero, promos, or outliers.  | Combination of data entry errors, phased pricing, or product repackaging. | Manually inspect. For automation, remove obvious typos, impute modal price, and flag for review or exclude from pricing models. |\n",
    "\n",
    "---\n",
    "# Data loss after cleaning is Data loss: 26.54%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Data Quality\n",
    "1. Data Tests\n",
    "I implemented multiple layers of testing to ensure accuracy and reliability of models:\n",
    "- Built-in dbt tests ‚Üí unique, not_null, accepted_values.\n",
    "- dbt_utils package ‚Üí extended SQL assertions such as data range checks and conditional validations.\n",
    "\n",
    "2. Source Freshness Checks\n",
    "- Validates that source tables are up-to-date and not stale.\n",
    "- Ensures downstream models are always built on the latest available data.\n",
    "\n",
    "\n",
    "#  Data Transformation \n",
    "\n",
    "## üîß Data Transformation ‚Äì `stg_transaction_logs`\n",
    "\n",
    "The `stg_transaction_logs` model standardizes raw transactional data to prepare it for accurate customer analytics. All transformations in this stage focus on **cleaning**, **filtering**, and **structuring** the data before it flows into intermediate and mart layers.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Was Transformed\n",
    "\n",
    "| Transformation                  | Description                                                                                                                                                        |\n",
    "| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Business Logic Exclusions**   | Removed invalid transactions (e.g. free samples, returns, manual adjustments, debt, damaged or lost products postage) using clear rules (see [Business Logic Exclusions](#)).                     |\n",
    "| **Country Mapping & Filtering** | - Excluded `European Community` and `Unspecified` entries. <br> - Standardized country names using a **dbt seed file** to map raw values like `EIRE` to `Ireland`, `RSA` to `South Africa`, `USA` to `United States` and `Channel Islands` to `United Kingdom`. |\n",
    "| **Data Type Casting**           | Cast all columns to consistent and analysis-ready formats (e.g. `InvoiceNo` as `varchar`, `UnitPrice` as `numeric(12,2)`, `InvoiceDate` as `timestamp`).           |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why These Transformations Were Necessary\n",
    "\n",
    "* **Ensure analytical integrity**: Removes noise and non-customer behavior from modeling (e.g., CLTV, churn).\n",
    "* **Support regional analysis**: Clean, consistent country names enable geographic segmentation and filtering.\n",
    "* **Enforce schema consistency**: Proper data types improve query performance, downstream joins, and dashboard compatibility.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **Business Logic Filtering**\n",
    "\n",
    "Excluded rows using SQL logic in the dbt model:\n",
    "\n",
    "```sql\n",
    "WHERE UnitPrice > 0\n",
    "  AND Quantity > 0\n",
    "  AND Description IS NOT NULL\n",
    "  AND NOT Description ILIKE ANY (ARRAY['%damaged%', '%adjust%', '%manual%', ...])\n",
    "```\n",
    "### üì¶ 1. **Zero-Value Transactions (Free or Display Items)**\n",
    "\n",
    "**Condition:**\n",
    "\n",
    "* `UnitPrice = 0`\n",
    "* `Quantity >= 0`\n",
    "* `CustomerID IS NOT NULL`\n",
    "* `InvoiceNo NOT LIKE 'C%'`\n",
    "* `Description` does **not** contain:\n",
    "  `'adjust'`, `'manual'`, `'check'`, `'Adjust bad debt'`, `'???'`, `'missing'`, `'POST'`, `'DOT'`, `'CRUK'`, `'BANK CHARGES'`\n",
    "* `StockCode NOT IN ('M', 'C2')`\n",
    "\n",
    "**Action:**\n",
    "Excluded from revenue, CLTV, churn, segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ 2. **Returns & Refunds**\n",
    "\n",
    "**Condition:**\n",
    "\n",
    "* `Quantity < 0`\n",
    "* OR `InvoiceNo LIKE 'C%'`\n",
    "\n",
    "**Action:**\n",
    "Included in net revenue; excluded from CLTV and behavioral models.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ 3. **Inventory Adjustments & Manual Ops Corrections**\n",
    "\n",
    "**Condition:**\n",
    "\n",
    "* `StockCode IN ('M', 'C2')`\n",
    "* OR `Description` contains `'adjust'`, `'manual'`, `'check'`\n",
    "* AND `Description != 'Adjust bad debt'`\n",
    "* AND not matched to group 2\n",
    "\n",
    "**Action:**\n",
    "Excluded from revenue, CLTV, segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "### üí∏ 4. **Bad Debt & Write-Offs**\n",
    "\n",
    "**Condition:**\n",
    "\n",
    "* `Description = 'Adjust bad debt'` (case-insensitive exact/fuzzy match)\n",
    "\n",
    "**Action:**\n",
    "Excluded from all behavioral models; retained for financial audit.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì 5. **Incomplete or Suspicious Entries**\n",
    "\n",
    "**Condition:**\n",
    "\n",
    "* `CustomerID IS NULL AND UnitPrice = 0`\n",
    "* OR `Description IN ('???', 'missing', 'wrongly sold')`\n",
    "\n",
    "**Action:**\n",
    "Flagged or dropped depending on QA policy.\n",
    "\n",
    "---\n",
    "\n",
    "### üõí 6. **Non-Product / Operational Charges**\n",
    "\n",
    "**Condition:**\n",
    "\n",
    "* `Description` contains any of:\n",
    "  `'POST'`, `'DOT'`, `'MAILOUT'`, `'BANK CHARGES'`, `'CRUK'`\n",
    "* AND not already classified above\n",
    "\n",
    "**Action:**\n",
    "Excluded from SKU-level sales, CLTV, segmentation.\n",
    "\n",
    "#### 2. **Country Mapping via dbt Seed File**\n",
    "\n",
    "Used a seed file (`country_mapping.csv`) joined into the model to standardize country names:\n",
    "\n",
    "```sql\n",
    "-- Sample of the seed file\n",
    "raw_country         | mapped_country\n",
    "--------------------|----------------\n",
    "EIRE                | Ireland\n",
    "RSA                 | South Africa\n",
    "Channel Islands     | United Kingdom\n",
    "USA                 | United States\n",
    "\n",
    "-- Join in dbt model\n",
    "LEFT JOIN {{ ref('country_mapping') }} cm\n",
    "  ON lower(trim(country)) = lower(trim(cm.raw_country))\n",
    "```\n",
    "\n",
    "#### 3. **Casting Data Types**\n",
    "\n",
    "Ensured compatibility and clean schema:\n",
    "\n",
    "```sql\n",
    "CAST(InvoiceNo   AS VARCHAR)        AS InvoiceNo,\n",
    "CAST(StockCode   AS VARCHAR)        AS StockCode,\n",
    "CAST(Description AS VARCHAR)        AS Description,\n",
    "CAST(cm.mapped_country AS VARCHAR)  AS Country,\n",
    "CAST(UnitPrice   AS NUMERIC(12,2))  AS UnitPrice,\n",
    "CAST(Quantity    AS INT)            AS Quantity,\n",
    "CAST(CustomerID  AS INT)            AS CustomerID,\n",
    "CAST(InvoiceDate AS TIMESTAMP)      AS InvoiceDate\n",
    "```\n",
    "\n",
    "\n",
    "## üîß Data Transformation ‚Äì `int_total_amount_calc`\n",
    "\n",
    "The `int_total_amount_calc` model computes transaction-level revenue by calculating the **total amount spent per line item**. This forms the foundational layer for all **CLTV**, **AOV**, and **segmentation metrics** downstream.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Was Transformed\n",
    "\n",
    "| Transformation               | Description                                                                                                             |\n",
    "| ---------------------------- | ----------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Total Amount Calculation** | Computed `TotalAmount` as the product of `Quantity * UnitPrice`, representing the gross amount paid for each line item. |\n",
    "| **Selected Key Columns**     | Retained only relevant columns for revenue and customer-level analysis.                                                 |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why This Transformation Was Necessary\n",
    "\n",
    "* **Enable monetary aggregation**: The `TotalAmount` field allows us to roll up revenue by customer and invoice.\n",
    "* **Support downstream KPIs**: This is a required metric for computing AOV, CLTV, RFM, churn, and segmentation models.\n",
    "* **Reduce row complexity**: By selecting only the required fields, the model remains lean and focused on financial analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **TotalAmount Calculation**\n",
    "\n",
    "The core transformation is the line-level amount:\n",
    "\n",
    "```sql\n",
    "Quantity * UnitPrice AS TotalAmount\n",
    "```\n",
    "\n",
    "This accurately reflects what a customer paid for each product unit on each invoice line.\n",
    "\n",
    "#### 2. **Column Selection**\n",
    "\n",
    "Only the necessary columns were retained to support clean joins and aggregations:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    CustomerID,\n",
    "    InvoiceNo,\n",
    "    InvoiceDate,\n",
    "    Description,\n",
    "    Quantity,\n",
    "    UnitPrice,\n",
    "    Quantity * UnitPrice AS TotalAmount,\n",
    "    Country\n",
    "FROM {{ ref('stg_transaction_logs') }}\n",
    "```\n",
    "\n",
    "## üîß Data Transformation ‚Äì `int_customer_aov`\n",
    "\n",
    "The `int_customer_aov` model calculates the **Average Order Value (AOV)** per customer, based on all their historical transactions. This metric is a key input into customer segmentation ie. CLTV modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Was Transformed\n",
    "\n",
    "| Transformation                 | Description                                                                       |\n",
    "| ------------------------------ | --------------------------------------------------------------------------------- |\n",
    "| **Customer-Level Aggregation** | Grouped transactions by `CustomerID` and calculated the average of `TotalAmount`. |\n",
    "| **AOV Calculation**            | Computed `CustomerAOV` using `AVG(TotalAmount)` per customer.                     |\n",
    "| **Field Selection**            | Retained only `CustomerID` and `CustomerAOV` for lean, focused output.            |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why This Transformation Was Necessary\n",
    "\n",
    "* **Measure purchase behavior**: AOV reflects how much, on average, each customer spends per transaction.\n",
    "* **Enable value-based segmentation**: Helps classify customers into high-, medium-, or low-spenders.\n",
    "* **Support CLTV modeling**: A core component in lifetime value estimation (LTV = AOV √ó purchase frequency √ó lifespan x gross margin).\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **AOV Calculation per Customer**\n",
    "\n",
    "The transformation used a simple aggregation over the transactional data:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    CustomerID,\n",
    "    SUM(TotalAmount)/COUNT(InvoiceNo) AS CustomerAOV\n",
    "FROM {{ ref('int_total_amount_calc') }}\n",
    "GROUP BY CustomerID\n",
    "```\n",
    "\n",
    "#### 2. **Selected Fields**\n",
    "\n",
    "Only the fields needed for downstream joins and modeling were kept:\n",
    "\n",
    "* `CustomerID`\n",
    "* `CustomerAOV`\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Data Transformation ‚Äì `int_customer_cltv`\n",
    "\n",
    "The `int_customer_cltv` model calculates **Customer Lifetime Value (CLTV)** for each customer, expressed in both revenue and profit terms. This model combines behavioral metrics (frequency and tenure) with spending patterns (AOV) and business-level profitability (gross margin).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Was Transformed\n",
    "\n",
    "| Transformation                 | Description                                                                                          |\n",
    "| ------------------------------ | ---------------------------------------------------------------------------------------------------- |\n",
    "| **CLTV (Revenue) Calculation** | Estimated revenue per customer using: `CustomerAOV √ó Frequency √ó Tenure`.                            |\n",
    "| **CLTV (Profit) Calculation**  | Multiplied CLTV revenue by `gross_margin` to estimate lifetime profit contribution.                  |\n",
    "| **Field Selection**            | Retained only key identifiers and computed metrics: `CustomerID`, `CLTV (Revenue)`, `CLTV (Profit)`. |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why This Transformation Was Necessary\n",
    "\n",
    "* **Quantifies customer value**: Allows the business to prioritize high-value customers for marketing and retention.\n",
    "* **Supports strategic decision-making**: Used for offer targeting, segmentation, pricing, and churn mitigation.\n",
    "* **Enables profit-based modeling**: Distinguishes between top spenders and top contributors to actual bottom-line profit.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **Inputs Joined**\n",
    "\n",
    "Joined the following models:\n",
    "\n",
    "* `a` ‚Äì from `int_customer_aov`: contains `CustomerAOV`\n",
    "* `r` ‚Äì from `int_customer_behavior_metrics`: contains `Frequency` and `Tenure`\n",
    "* `c` ‚Äì from `ref('business_constants')` or config: provides `gross_margin`\n",
    "\n",
    "#### 2. **CLTV Revenue Formula**\n",
    "\n",
    "```sql\n",
    "CustomerAOV √ó Frequency √ó Tenure AS customer_cltv_revenue\n",
    "```\n",
    "\n",
    "#### 3. **CLTV Profit Formula**\n",
    "\n",
    "```sql\n",
    "CustomerAOV √ó Frequency √ó Tenure √ó gross_margin AS customer_cltv_profit\n",
    "```\n",
    "\n",
    "#### 4. **Final Output**\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    r.CustomerID,\n",
    "    a.CustomerAOV * r.Frequency * r.Tenure AS customer_cltv_revenue,\n",
    "    a.CustomerAOV * r.Frequency * r.Tenure * c.gross_margin AS customer_cltv_profit\n",
    "FROM {{ ref('int_customer_behavior_metrics') }} r\n",
    "JOIN {{ ref('int_customer_aov') }} a ON r.CustomerID = a.CustomerID\n",
    "CROSS JOIN {{ ref('business_constants') }} c\n",
    "```\n",
    "---\n",
    "\n",
    "## üîß Data Transformation ‚Äì `int_customer_rfm_values`\n",
    "\n",
    "The `int_customer_rfm_values` model calculates **RFM (Recency, Frequency, Monetary)** metrics for each customer. These are foundational for behavioral segmentation and customer value scoring.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Was Transformed\n",
    "\n",
    "| Metric        | Description                                                                                                        |\n",
    "| ------------- | ------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Recency**   | Days since the customer's most recent purchase: `DATEDIFF(day, MAX(InvoiceDate), snapshot_date)`                   |\n",
    "| **Tenure**    | Days between the customer's first purchase and the snapshot date: `DATEDIFF(day, MIN(InvoiceDate), snapshot_date)` |\n",
    "| **Frequency** | Total number of distinct purchases: `COUNT(DISTINCT InvoiceNo)`                                                    |\n",
    "| **Monetary**  | Total amount the customer has spent: `SUM(TotalAmount)`                                                            |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why This Transformation Was Necessary\n",
    "\n",
    "* **Recency** captures how recently the customer was active ‚Äî useful for churn detection.\n",
    "* **Frequency** reflects engagement and loyalty.\n",
    "* **Monetary** shows how valuable the customer is.\n",
    "* **Tenure** helps in calculating behavioral lifecycle metrics (e.g., CLTV time horizon).\n",
    "* Together, RFM is a **proven framework** for customer segmentation, lifecycle tracking, and targeting strategy.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **Join with Snapshot Date**\n",
    "\n",
    "The model uses a **snapshot date** (from a dbt seed or config) to anchor all time-based calculations:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    CustomerID,\n",
    "    DATEDIFF(day, MAX(InvoiceDate), s.snapshot_date) AS Recency,\n",
    "    DATEDIFF(day, MIN(InvoiceDate), s.snapshot_date) AS Tenure,\n",
    "    COUNT(DISTINCT InvoiceNo) AS Frequency,\n",
    "    SUM(TotalAmount) AS Monetary\n",
    "FROM {{ ref('int_total_amount_calc') }} t\n",
    "CROSS JOIN {{ ref('snapshot_date') }} s\n",
    "GROUP BY CustomerID, s.snapshot_date\n",
    "```\n",
    "\n",
    "\n",
    "## üîß Data Transformation ‚Äì `int_customer_dau_mau_stickiness`\n",
    "\n",
    "The `int_customer_dau_mau_stickiness` model calculates **engagement metrics** over time by tracking how many unique customers interact with the platform daily (DAU), over a rolling monthly window (MAU), and how \"sticky\" the product is (Stickiness = DAU / MAU).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Was Transformed\n",
    "\n",
    "| Metric         | Description                                                                                 |\n",
    "| -------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **DAU**        | Count of distinct `CustomerID`s per `InvoiceDate` ‚Äî representing **Daily Active Users**.    |\n",
    "| **MAU**        | 30-day rolling sum of DAU using a window function ‚Äî representing **Monthly Active Users**.  |\n",
    "| **Stickiness** | Ratio of DAU to MAU (`DAU / MAU`), rounded to two decimals ‚Äî measuring platform stickiness. |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why This Transformation Was Necessary\n",
    "\n",
    "* **Track engagement over time**: Understand how consistently customers return to shop.\n",
    "* **Benchmark activity health**: DAU/MAU Stickiness is a key SaaS/ecommerce metric to evaluate retention and product \"habit formation\".\n",
    "* **Enable time-based segmentation**: Helps separate loyal customers from one-time buyers.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **DAU Calculation**\n",
    "\n",
    "Calculated per day:\n",
    "\n",
    "```sql\n",
    "COUNT(DISTINCT CustomerID) AS DAU\n",
    "```\n",
    "\n",
    "#### 2. **MAU (30-day Rolling Window)**\n",
    "\n",
    "Computed using a window function:\n",
    "\n",
    "```sql\n",
    "SUM(DAU) OVER (\n",
    "    ORDER BY InvoiceDate\n",
    "    ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n",
    ") AS MAU\n",
    "```\n",
    "\n",
    "#### 3. **Stickiness Ratio**\n",
    "\n",
    "Handled division-by-zero safely:\n",
    "\n",
    "```sql\n",
    "ROUND(DAU / NULLIF(MAU, 0), 2) AS Stickiness\n",
    "```\n",
    "\n",
    "#### 4. **Selected Columns**\n",
    "\n",
    "Only core engagement metrics were kept:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    InvoiceDate,\n",
    "    DAU,\n",
    "    MAU,\n",
    "    ROUND(DAU / NULLIF(MAU, 0), 2) AS Stickiness\n",
    "FROM ...\n",
    "```\n",
    "---\n",
    "## üîß Data Transformation ‚Äì `int_customer_churn_flag`\n",
    "\n",
    "The `int_customer_churn_flag` model assigns a binary churn label to each customer based on their **Recency** ‚Äî the number of days since their last transaction ‚Äî as of a defined **snapshot date**.  \n",
    "\n",
    "---\n",
    "\n",
    "### üõ† Macro Definition\n",
    "\n",
    "```jinja\n",
    "{% macro churn_flag(recency_col, churn_window=90) %}\n",
    "    -- Returns TRUE if the customer is churned (Recency > churn_window), else FALSE.\n",
    "    ({{ recency_col }} > {{ churn_window }})\n",
    "{% endmacro %}\n",
    "\n",
    "```sql\n",
    "WITH churn_flag_table AS (\n",
    "    SELECT\n",
    "        CustomerID,\n",
    "        {{ churn_flag('Recency', var('churn_window', 90)) }} AS IsChurned\n",
    "    FROM {{ ref('int_customer_rfm_values') }}\n",
    ")\n",
    "SELECT *\n",
    "FROM churn_flag_table\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Data Transformation ‚Äì `int_customer_rfm_values`\n",
    "\n",
    "The `int_customer_rfm_score` model assigns **quantile-based scores (1‚Äì5)** to each customer across the three RFM dimensions ‚Äî **Recency**, **Frequency**, and **Monetary** ‚Äî enabling behavioral segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Was Transformed\n",
    "\n",
    "| Transformation      | Description                                                                                              |\n",
    "| ------------------- | -------------------------------------------------------------------------------------------------------- |\n",
    "| **Recency Score**   | Customers are ranked into 5 buckets using `ntile(5)` ‚Äî lower recency = higher score (i.e., more recent). |\n",
    "| **Frequency Score** | Higher frequency of purchase ‚Üí higher `f_score`. Scored using `ntile(5)`.                                |\n",
    "| **Monetary Score**  | Customers who spent more receive higher `m_score`, also via `ntile(5)`.                                  |\n",
    "| **Field Selection** | Retained only `CustomerID`, `r_score`, `f_score`, and `m_score`.                                         |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why This Transformation Was Necessary\n",
    "\n",
    "* **Enables RFM Segmentation**: Scoring simplifies raw metrics into interpretable levels of customer value.\n",
    "* **Supports customer targeting**: Identifies \"Champions\", \"At Risk\", \"Hibernating\", and other lifecycle stages.\n",
    "* **Enables modeling and visualization**: Scores can be grouped into RFM bands for easier cohort analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **Score Calculation with Window Functions**\n",
    "\n",
    "Using `ntile(5)` to assign quintile-based ranks:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    CustomerID,\n",
    "    NTILE(5) OVER (ORDER BY Recency ASC) AS r_score,   -- Lower recency ‚Üí higher score\n",
    "    NTILE(5) OVER (ORDER BY Frequency DESC) AS f_score, -- Higher frequency ‚Üí higher score\n",
    "    NTILE(5) OVER (ORDER BY Monetary DESC) AS m_score   -- Higher monetary ‚Üí higher score\n",
    "FROM {{ ref('int_customer_rfm_values') }}\n",
    "```\n",
    "\n",
    "#### 2. **Scoring Logic**\n",
    "\n",
    "* `r_score`: 5 = most recent, 1 = longest inactive\n",
    "* `f_score`: 5 = most frequent, 1 = least frequent\n",
    "* `m_score`: 5 = highest spender, 1 = lowest spender\n",
    "\n",
    "## üîß Data Transformation ‚Äì `int_customer_rfm_score`\n",
    "\n",
    "The `int_customer_rfm_score` model assigns **quantile-based scores (1‚Äì5)** to each customer across the three RFM dimensions ‚Äî **Recency**, **Frequency**, and **Monetary** ‚Äî enabling behavioral segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Was Transformed\n",
    "\n",
    "| Transformation      | Description                                                                                              |\n",
    "| ------------------- | -------------------------------------------------------------------------------------------------------- |\n",
    "| **Recency Score**   | Customers are ranked into 5 buckets using `ntile(5)` ‚Äî lower recency = higher score (i.e., more recent). |\n",
    "| **Frequency Score** | Higher frequency of purchase ‚Üí higher `f_score`. Scored using `ntile(5)`.                                |\n",
    "| **Monetary Score**  | Customers who spent more receive higher `m_score`, also via `ntile(5)`.                                  |\n",
    "| **Field Selection** | Retained only `CustomerID`, `r_score`, `f_score`, and `m_score`.                                         |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why This Transformation Was Necessary\n",
    "\n",
    "* **Enables RFM Segmentation**: Scoring simplifies raw metrics into interpretable levels of customer value.\n",
    "* **Supports customer targeting**: Identifies \"Champions\", \"At Risk\", \"Hibernating\", and other lifecycle stages.\n",
    "* **Enables modeling and visualization**: Scores can be grouped into RFM bands for easier cohort analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **Score Calculation with Window Functions**\n",
    "\n",
    "Using `ntile(5)` to assign quintile-based ranks:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    CustomerID,\n",
    "    NTILE(5) OVER (ORDER BY Recency ASC) AS r_score,   -- Lower recency ‚Üí higher score\n",
    "    NTILE(5) OVER (ORDER BY Frequency DESC) AS f_score, -- Higher frequency ‚Üí higher score\n",
    "    NTILE(5) OVER (ORDER BY Monetary DESC) AS m_score   -- Higher monetary ‚Üí higher score\n",
    "FROM {{ ref('int_customer_rfm_values') }}\n",
    "```\n",
    "\n",
    "#### 2. **Scoring Logic**\n",
    "\n",
    "* `r_score`: 5 = most recent, 1 = longest inactive\n",
    "* `f_score`: 5 = most frequent, 1 = least frequent\n",
    "* `m_score`: 5 = highest spender, 1 = lowest spender\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Data Transformation ‚Äì `int_customer_cltv_segmentation`\n",
    "\n",
    "The `int_customer_cltv_segmentation` model classifies each customer into a **CLTV profit segment** ‚Äî `\"Low\"`, `\"Medium\"`, or `\"High\"` ‚Äî based on their **lifetime profit contribution**. This segmentation enables targeted marketing, retention strategies, and prioritization of high-value customers.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Was Transformed\n",
    "\n",
    "| Transformation            | Description                                                                                  |\n",
    "| ------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| **Percentile Thresholds** | Computed the 33rd and 66th percentiles of `customer_cltv_profit` across all customers.       |\n",
    "| **CLTV Segmentation**     | Assigned each customer to a `cltv_profit_segmentation` bucket using the computed thresholds. |\n",
    "| **Output Fields**         | Retained only `CustomerID` and the resulting segmentation label.                             |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why This Transformation Was Necessary\n",
    "\n",
    "* **Simplifies customer value**: Converts continuous CLTV values into discrete, explainable tiers.\n",
    "* **Enables actionability**: Empowers marketers and product teams to treat high-, medium-, and low-value customers differently.\n",
    "* **Supports modeling and dashboarding**: Useful as a feature in churn prediction, lifetime value modeling, or retention dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **Threshold Calculation**\n",
    "\n",
    "Used a subquery (`threshold`) to calculate the 33rd and 66th percentiles:\n",
    "\n",
    "```sql\n",
    "percentile_cont(0.33) WITHIN GROUP (ORDER BY customer_cltv_profit) AS p33,\n",
    "percentile_cont(0.66) WITHIN GROUP (ORDER BY customer_cltv_profit) AS p66\n",
    "```\n",
    "\n",
    "#### 2. **CLTV Segmentation Logic**\n",
    "\n",
    "Assigned each customer to a segment based on their `customer_cltv_profit` value:\n",
    "\n",
    "```sql\n",
    "CASE  \n",
    "  WHEN customer_cltv_profit < p33 THEN 'Low'\n",
    "  WHEN customer_cltv_profit < p66 THEN 'Medium'\n",
    "  ELSE 'High'\n",
    "END AS cltv_profit_segmentation\n",
    "```\n",
    "\n",
    "#### 3. **Final Output**\n",
    "\n",
    "Joined the threshold values using `CROSS JOIN` to apply them across the customer table:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "  CustomerID,\n",
    "  cltv_profit_segmentation\n",
    "FROM int_customer_cltv\n",
    "CROSS JOIN threshold\n",
    "```\n",
    "\n",
    "## üîß Data Transformation ‚Äì `int_customer_churn_segmentation`\n",
    "\n",
    "The `int_customer_churn_segmentation` model assigns each customer to a **churn risk group** (`Low`, `Medium`, `High`, or `Churned`) based on their **recency of activity** relative to a predefined set of thresholds.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Was Transformed\n",
    "\n",
    "| Transformation             | Description                                                                                                                             |\n",
    "| -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Churn Group Assignment** | Used the `Recency` value and a churn flag (`IsChurned`) to classify customers into churn risk tiers.                                    |\n",
    "| **Threshold Mapping**      | Applied configurable recency thresholds for `Low`, `Medium`, and `High` churn risk levels using a dbt seed file (`churn_risk_mapping`). |\n",
    "| **Output Fields**          | Returned `CustomerID` and the assigned `churn_risk_group`.                                                                              |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why This Transformation Was Necessary\n",
    "\n",
    "* **Enables proactive retention**: Helps identify users at different levels of churn risk, even before they fully churn.\n",
    "* **Supports lifecycle segmentation**: Adds an interpretable behavioral tier based on customer inactivity.\n",
    "* **Feeds dashboards and targeting**: Used in marketing, lifecycle campaigns, and churn mitigation reporting.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **Inputs**\n",
    "\n",
    "* `int_customer_churn_flag`: provides `CustomerID` and churn flag (`IsChurned`)\n",
    "* `int_customer_rfm_values`: provides `Recency` per customer\n",
    "* `churn_risk_mapping` (seed file): defines cutoff thresholds for `Low`, `Medium`, and `High` risk\n",
    "\n",
    "Example contents of `churn_risk_mapping`:\n",
    "\n",
    "| low\\_risk | medium\\_risk | high\\_risk |\n",
    "| --------- | ------------ | ---------- |\n",
    "| 30        | 60           | 90         |\n",
    "\n",
    "#### 2. **Churn Risk Classification Logic**\n",
    "\n",
    "```sql\n",
    "CASE\n",
    "  WHEN c.IsChurned = 'TRUE' THEN 'Churned'\n",
    "  WHEN r.Recency <= m.low_risk THEN 'Low'\n",
    "  WHEN r.Recency <= m.medium_risk THEN 'Medium'\n",
    "  WHEN r.Recency <= m.high_risk THEN 'High'\n",
    "END AS churn_risk_group\n",
    "```\n",
    "\n",
    "#### 3. **Final Join and Selection**\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    c.CustomerID,\n",
    "    <churn risk logic>\n",
    "FROM {{ ref('int_customer_churn_flag') }} c\n",
    "LEFT JOIN {{ ref('int_customer_rfm_values') }} r ON r.CustomerID = c.CustomerID\n",
    "CROSS JOIN {{ ref('churn_risk_mapping') }} m\n",
    "```\n",
    "\n",
    "## üîß Data Transformation ‚Äì `int_customer_rfm_segmentation`\n",
    "\n",
    "The `int_customer_rfm_segmentation` model assigns each customer to a **behavioral segment** based on their **RFM scores** (`r_score`, `f_score`, `m_score`). This model maps numerical RFM profiles to human-readable labels like \"Champions\", \"At Risk\", or \"Hibernating\" using a predefined ruleset.\n",
    "\n",
    "\n",
    "\n",
    " ‚úÖ What Was Transformed\n",
    "\n",
    "| Transformation              | Description                                                                                      |\n",
    "| --------------------------- | ------------------------------------------------------------------------------------------------ |\n",
    "| **RFM Segment Assignment**  | Mapped customers' individual RFM scores to predefined segment labels using a seed-based ruleset. |\n",
    "| **Join with Mapping Table** | Matched RFM scores to ranges defined in `rfm_segment_mapping` (a dbt seed file).                 |\n",
    "| **Output Fields**           | Returned only `CustomerID` and `rfm_segmentation` label.                                         |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why This Transformation Was Necessary\n",
    "\n",
    "* **Adds business-friendly segmentation**: Converts numeric scores into interpretable customer groups.\n",
    "* **Enables targeting strategies**: Used for marketing campaigns, retention efforts, and upsell strategies.\n",
    "* **Supports customer lifecycle modeling**: Offers insights into engagement level and purchase behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è How It Was Done\n",
    "\n",
    "#### 1. **Inputs**\n",
    "\n",
    "* `int_customer_rfm_score`: contains customer-level RFM scores (`r_score`, `f_score`, `m_score`)\n",
    "* `rfm_segment_mapping`: a seed file that defines scoring ranges and their corresponding segment labels\n",
    "\n",
    "Example of `rfm_segment_mapping`:\n",
    "\n",
    "| min\\_r | max\\_r | min\\_f | max\\_f | min\\_m | max\\_m | rfm\\_segmentation |\n",
    "| ------ | ------ | ------ | ------ | ------ | ------ | ----------------- |\n",
    "| 4      | 5      | 4      | 5      | 4      | 5      | Champions         |\n",
    "| 1      | 2      | 1      | 2      | 1      | 2      | Hibernating       |\n",
    "\n",
    "#### 2. **Segmentation Logic**\n",
    "\n",
    "Joined each customer‚Äôs RFM scores against the segmentation rules:\n",
    "\n",
    "```sql\n",
    "ON r_score BETWEEN seg.min_r AND seg.max_r\n",
    "AND f_score BETWEEN seg.min_f AND seg.max_f\n",
    "AND m_score BETWEEN seg.min_m AND seg.max_m\n",
    "```\n",
    "\n",
    "#### 3. **Final Output**\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    rfm.CustomerID,\n",
    "    rfm_segmentation\n",
    "FROM {{ ref('int_customer_rfm_score') }} rfm\n",
    "LEFT JOIN {{ ref('rfm_segment_mapping') }} seg\n",
    "  ON rfm.r_score BETWEEN seg.min_r AND seg.max_r\n",
    " AND rfm.f_score BETWEEN seg.min_f AND seg.max_f\n",
    " AND rfm.m_score BETWEEN seg.min_m AND seg.max_m\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  Business Intelligence Dashboard\n",
    "Click here to view dashboard: [Dashboard](https://customer-segmentation-rfm-cltv-churn.streamlit.app/) *(*Note: Might take a minute to load*)*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Data Modeling\n",
    "### Final mart tables products\n",
    "#### 1. OBT (One big table)\n",
    "#### 2. Star Schema\n",
    "\n",
    "1. One Big Table (OBT)\n",
    "The final output is structured as a single, wide table that is fully denormalized and flattened at the customer level. \n",
    "- Used for\n",
    "  - ML Models \n",
    "  - Dashboards / BI Tools \n",
    "  - Ingestion / Export \n",
    "2. One Big Table (OBT)\n",
    "A normalized model with a central fact table (e.g., fct_customer_metrics) surrounded by dimension tables (e.g., dim_churn_group, dim_rfm_group)- Used for\n",
    "  - Data Analysts\n",
    "  - Performance in Data Warehouse \n",
    "\n",
    "\n",
    "| Layer                  | Modeling Style           | Purpose                                                                                             |\n",
    "| ---------------------- | ------------------------ | --------------------------------------------------------------------------------------------------- |\n",
    "| `stg_` (Staging)       | **Normalized**           | One-to-one cleanup of raw source data (no joins or heavy logic)                                     |\n",
    "| `int_` (Intermediate)  | **Modular + Normalized** | Each metric calculated in isolation (AOV, DAU/MAU, CLTV, etc.) with narrow outputs                  |\n",
    "| `mart_` (Final Output) | **OBT and star schema**         | Wide, joined tables with all key KPIs and labels for direct consumption by dashboards and ML models |\n",
    "\n",
    "\n",
    "‚úÖ Why This Was the Right Choice\n",
    "‚úÖ BI-Friendly: Denormalized marts reduce complexity for downstream users, ie no excessive joins (analysts, dashboards).\n",
    "\n",
    "‚úÖ Performance: Avoids excessive joins during query time by materializing wide, pre-joined tables.\n",
    "\n",
    "‚úÖ Modularity: The intermediate models are modular, reusable, and testable because they follow a normalized structure. Each model focuses on a single business metric (e.g., AOV, CLTV, Recency) and retains a clear key-to-metric relationship ‚Äî typically using CustomerID as the grain. This design allows each intermediate view to be plugged into multiple downstream marts without redundancy, promoting maintainability and scalability across the analytics workflow.\n",
    "\n",
    "### Incremental models\n",
    "| **Table**                            | **Type**           | **Reason**                                                                                                                        |\n",
    "| ------------------------------------ | ------------------ | --------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `obt_customer_dau_mau_stickiness`    | `insert_overwrite` | Daily/Monthly activity can be recalculated at the partition level (by date), so overwriting just the new partitions is efficient. |\n",
    "| `obt_customer_kpis_and_segmentation` | `merge`            | KPIs and segmentation depend on evolving customer attributes, so merging ensures updates while avoiding full rebuilds.            |\n",
    "| `obt_overall_customer_kpis`          | `insert_overwrite` | Overall KPIs are aggregated by time, making partition overwrite the simplest and most efficient refresh method.                   |\n",
    "| `fct_customer_metrics`               | `merge`            | Fact tables grow over time but may also update when late-arriving data comes in, so merge handles inserts + updates well.         |\n",
    "| `dim_churn_group`                    | `merge (SCD 1)`    | Churn group dimension is updated with current state only (no history), so merge replaces old values efficiently.                  |\n",
    "| `dim_cltv_group`                     | `merge (SCD 1)`    | CLTV group dimension also reflects only the latest grouping, requiring merge for updating rows.                                   |\n",
    "| `dim_rfm_group`                      | `merge (SCD 1)`    | RFM segments can change with new activity, so merge ensures existing rows are updated in place.                                   |\n",
    "\n",
    "\n",
    "##### Why Incremental Models?\n",
    "\n",
    "Performance ‚Äì Instead of recalculating the entire dataset every run, only new or updated records are processed. This saves time and compute cost.\n",
    "\n",
    "Scalability ‚Äì As data volume grows (millions or billions of rows), full refreshes become impractical. Incremental logic ensures pipelines scale.\n",
    "\n",
    "Late-arriving data ‚Äì Real-world data often lands late (e.g., delayed transactions). Incremental models (especially with merge) allow efficient backfilling without full rebuilds.\n",
    "\n",
    "Flexibility ‚Äì Different strategies (insert_overwrite vs. merge) let you choose the right trade-off: overwrite clean partitions for time-based data, or merge updates for mutable dimensions/facts.\n",
    "\n",
    "dbt Best Practice ‚Äì In modern data stacks, incremental models are considered a must-have pattern for production-grade warehouses (Snowflake, BigQuery, Redshift) to optimize pipelines.\n",
    "\n",
    "##### What was left views \n",
    "- staging & intermediate models \n",
    "\n",
    "\n",
    "### üèóÔ∏è Why Staging & Intermediate Models Are Views ‚Äî and Marts Are Tables\n",
    "\n",
    "- The **staging (`stg_`) and intermediate (`int_`) models are materialized as views** because:\n",
    "    - They are **not queried directly by analysts or BI tools**.\n",
    "    - They need to be **dynamically recalculated** each day as new data arrives.\n",
    "    - They act as **reusable transformation layers** that feed multiple downstream marts.\n",
    "- In contrast, the **mart models (`mart_`) are materialized as tables** because:\n",
    "    - They serve as **final, analysis-ready datasets** used by analysts and visualized in dashboards.\n",
    "    - They are treated as **snapshots of the data at a specific point in time**, which ensures consistency in reporting.\n",
    "    - Materializing as tables improves **performance and stability** for tools consuming large, aggregated datasets.\n",
    "\n",
    "\n",
    "## Data lineage:\n",
    "![A description of my image](lineage.jpg)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "üîÅ Arrows = Data Dependencies\n",
    "Each arrow means:\n",
    "\n",
    "\"This model depends on the model it‚Äôs pointing from.\"\n",
    "In dbt terms: the downstream model has a ref() to the upstream model.\n",
    "\n",
    "## Data Orchestration\n",
    "- **Cron jobs** ‚Üí Managed with Dagster cron jobs, which trigger dbt runs for each model on a defined schedule.  \n",
    "- **Partitioning: `obt_customer_dau_mau_stickiness`** ‚Üí  \n",
    "  - This model is **partitioned in BigQuery by `InvoiceDate`**.  \n",
    "  - Partitioning was chosen instead of standard incrementals to allow **efficient backfilling** of past dates in case a build failed \n",
    "\n",
    "## CI/CD\n",
    "#### üîÑ CI/CD ‚Äì State-Aware dbt Builds\n",
    "- CI/CD pipelines are orchestrated with **GitHub Actions**.  \n",
    "- dbt is configured to use **state-based testing/building** (also called **state-aware CI/CD**):  \n",
    "  - Only models that have been **modified** (plus their downstream dependencies) are rebuilt.  \n",
    "  - All other models are **deferred** to the existing production state.  \n",
    "\n",
    "‚úÖ Benefits:  \n",
    "- **Faster builds** ‚Äì avoids rebuilding the entire DAG.  \n",
    "- **Cost-efficient** ‚Äì saves compute in BigQuery/Snowflake.  \n",
    "- **Safe & Scalable** ‚Äì tests only changes while relying on trusted production artifacts.  \n",
    "\n",
    "---\n",
    "\n",
    "#### ‚òÅÔ∏è GCP Container Registry Integration\n",
    "\n",
    "- All Docker images used for dbt and Dagster are **built and pushed** to **Google Cloud Container Registry (GCR)**.  \n",
    "- GitHub Actions workflows automatically:  \n",
    "  1. **Build** the Docker image.  \n",
    "  2. **Tag** with commit SHA or release version.  \n",
    "  3. **Push** to GCP Container Registry.  \n",
    "- This ensures a consistent, versioned image is always available for:  \n",
    "  - **Local dev** (developers can pull the same image used in production).  \n",
    "  - **CI pipelines** (GitHub Actions runs the same container image).  \n",
    "  - **Production orchestration** (Dagster pulls the latest tested container).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project transformed raw POS transaction data into actionable customer intelligence by implementing a modular, testable dbt pipeline across staging, intermediate, and mart layers. I engineered key customer metrics ‚Äî including **AOV**, **CLTV**, **RFM**, **churn**, and **engagement KPIs** ‚Äî and segmented the customer base for more targeted, data-driven marketing.\n",
    "\n",
    "Our final analytics layer enabled:\n",
    "\n",
    "* Clear **prioritization** of high-value and at-risk customers\n",
    "* Support for **personalized offers and retention strategies**\n",
    "* Quantification of **customer lifetime value and churn risk**\n",
    "* Generation of **marketing KPIs** like DAU, MAU, and Stickiness\n",
    "\n",
    "This reusable architecture supports continuous updates and is built with production best practices in mind: version-controlled, documented, and testable. By leveraging tools like **dbt, Snowflake, Plotly, and Streamlit**, the pipeline enables both analysts and marketers to make faster, smarter decisions ‚Äî with confidence in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### üîú **Next Steps**\n",
    "\n",
    "* Integrate campaign performance and uplift modeling\n",
    "* Expand segmentation with behavioral clusters (e.g., NMF or k-means)\n",
    "* Apply time-series forecasting on engagement and CLTV\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
